{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  word to vector in python\n",
    "\n",
    "import codecs\n",
    "import glob\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import pprint\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim.models.word2vec as w2v\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s',level= logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Akkash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Akkash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus is 137628 characters long\n"
     ]
    }
   ],
   "source": [
    "corpus_raw =u\"\"\n",
    "with codecs.open('kafka.txt',\"r\",\"utf-8\") as book_file:\n",
    "    corpus_raw+= book_file.read()\n",
    "    print('corpus is {0} characters long'.format(len(corpus_raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "raw_sentence = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_word(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \",raw)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence = []\n",
    "for raw_sent in raw_sentence:\n",
    "    if len(raw_sent) > 0:\n",
    "        sentence.append(sentence_to_word(raw_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It wasn't a dream.\n"
     ]
    }
   ],
   "source": [
    "print raw_sentence[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'It', u'wasn', u't', u'a', u'dream']\n"
     ]
    }
   ],
   "source": [
    "print sentence_to_word(raw_sentence[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Word 2 Vec\n",
    "\n",
    "num_features = 300\n",
    "min_word_count =3\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "context_size = 7\n",
    "\n",
    "downsampling = 1e-3\n",
    "\n",
    "seed=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kafka2vec = w2v.Word2Vec(sg=1,seed=seed,workers=num_workers,size=num_features,min_count=min_word_count,\n",
    "                         window=context_size,sample=downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-02 12:13:15,338: INFO: collecting all words and their counts\n",
      "2018-02-02 12:13:15,339: INFO: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-02-02 12:13:15,354: INFO: collected 3229 word types from a corpus of 25415 raw words and 917 sentences\n",
      "2018-02-02 12:13:15,355: INFO: Loading a fresh vocabulary\n",
      "2018-02-02 12:13:15,362: INFO: min_count=3 retains 1118 unique words (34% of original 3229, drops 2111)\n",
      "2018-02-02 12:13:15,362: INFO: min_count=3 leaves 22814 word corpus (89% of original 25415, drops 2601)\n",
      "2018-02-02 12:13:15,374: INFO: deleting the raw counts dictionary of 3229 items\n",
      "2018-02-02 12:13:15,375: INFO: sample=0.001 downsamples 72 most-common words\n",
      "2018-02-02 12:13:15,375: INFO: downsampling leaves estimated 15494 word corpus (67.9% of prior 22814)\n",
      "2018-02-02 12:13:15,375: INFO: estimated required memory for 1118 words and 300 dimensions: 3242200 bytes\n",
      "2018-02-02 12:13:15,385: INFO: resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "kafka2vec.build_vocab(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka2vec.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"trained\"):\n",
    "    os.makedirs(\"trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-02 12:13:47,756: INFO: saving Word2Vec object under trained\\kafka2v.w2v, separately None\n",
      "2018-02-02 12:13:47,759: INFO: not storing attribute syn0norm\n",
      "2018-02-02 12:13:47,762: INFO: not storing attribute cum_table\n",
      "2018-02-02 12:13:47,788: INFO: saved trained\\kafka2v.w2v\n"
     ]
    }
   ],
   "source": [
    "kafka2vec.save(os.path.join(\"trained\", \"kafka2v.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-02 12:17:48,016: INFO: loading Word2Vec object from trained\\kafka2v.w2v\n",
      "2018-02-02 12:17:48,035: INFO: loading wv recursively from trained\\kafka2v.w2v.wv.* with mmap=None\n",
      "2018-02-02 12:17:48,035: INFO: setting ignored attribute syn0norm to None\n",
      "2018-02-02 12:17:48,038: INFO: setting ignored attribute cum_table to None\n",
      "2018-02-02 12:17:48,040: INFO: loaded trained\\kafka2v.w2v\n"
     ]
    }
   ],
   "source": [
    "kafka2vec = w2v.Word2Vec.load(os.path.join(\"trained\", \"kafka2v.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne = sklearn.manifold.TSNE(n_components=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_vec_word = kafka2vec.syn0_lockf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vec_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'forced', 0.17502176761627197),\n",
       " (u'www', 0.1726372092962265),\n",
       " (u'official', 0.16279907524585724),\n",
       " (u'their', 0.1627403199672699),\n",
       " (u'these', 0.16127660870552063),\n",
       " (u'come', 0.1602317839860916),\n",
       " (u'behind', 0.15903502702713013),\n",
       " (u'current', 0.15684396028518677),\n",
       " (u'applicable', 0.15366685390472412),\n",
       " (u'eBooks', 0.15197433531284332)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka2vec.wv.most_similar('strain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
